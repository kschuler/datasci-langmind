---
title: "Model Specification"
subtitle: "Data Science for Studying Language and the Mind"
author: Katie Schuler
echo: true
format: 
    revealjs:
        chalkboard: true
        slide-number: true
        incremental: true 
        footer: "[https://kschuler.github.io/datasci-langmind/](/index.html)"
---

```{r}
#| echo: false
library(tidyverse)
library(infer)
library(languageR)
library(mosaic)
theme_set(theme_classic(base_size = 20))
set.seed(123)

```

# Tuesday

## Announcements {.smaller}



## You are `here` {.smaller} 

:::: {.columns}

::: {.column width="33%"}

##### Data science with R 
::: {.nonincremental}
- R basics
- Data visualization
- Data wrangling
:::
:::

::: {.column width="33%"}

##### Stats & Model building
::: {.nonincremental}
- Sampling distribution
- Hypothesis testing
- `Model specification`
- Model fitting 
- Model accuracy
- Model reliability
:::
:::

::: {.column width="33%"}

##### More advanced 
::: {.nonincremental}

- Classification

::: 
:::

::::


# Review 

Sampling distribution and hypothesis testing with Correlation!

## Exploring relationships {.smaller}

To review what we learned before break, let's explore the relationship between Frequency and meanFamiliarity in the `ratings` dataset of the `languageR` package. 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point()
```

## Is there a relationship?  {.smaller}

If there was no relationship, we'd say there are **independent**: knowing the value of one provides no information about about the other. But that's not the case here.

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point()
```

## Yes, a linear relationship {.smaller}

In a linear relationship, when one variable goes up the other goes up (positive); or when one goes up the other goes down (negative). 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point() +
    geom_smooth(color = "blue", method = "lm", se = FALSE)
```

## Quantify with correlation {.smaller}

One way to quantify linear relationships is with **correlation** ($r$). Correlation expresses the linear relationship as a range from -1 (perfectly negative) to 1 (perfectly positive). 

![](/assests/images/correlation.jpeg)

## Computing correlation in R {.smaller}

We can compute a correlation with R's built in `cor(x,y)` function

. . . 

```{r}
#| output-location: column

cor(
  x = ratings$Frequency, 
  y = ratings$meanFamiliarity
)
```

. . . 

Or via the `infer` pacakge. 

. . . 

```{r}
#| output-location: column

(obs_corr <- ratings %>%
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  calculate(stat = "correlation"))
```

## Correlation uncertainty {.smaller}

Just like the mean ‚Äî and all other test statistics! ‚Äî $r$ is subject to sampling variability. We can indicate our uncertainty around the correlation the same way we always have:

. . . 

Construct the sampling distribution for the correlation: 

. . . 
```{r}
#| output-location: column

sampling_distribution <- ratings %>%
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "correlation")

head(sampling_distribution)
```

. . . 

Compute a confidence interval 

. . . 

```{r}
#| output-location: column

ci <- sampling_distribution %>% 
  get_ci(level = 0.95, type = "percentile")

ci
```

## üí™ In-class Exercise {.smaller}

*Take a few minutes to try this yourself!*

Use the `infer` way to visualize the sampling distribution and shade the confidence interval we just computed. Change the x-axis label to **stat (correlation)** as pictured below. 

. . . 

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 5

sampling_distribution %>%
  visualize() + 
  shade_ci(ci) +
  labs(x = "stat (correlation)")
```

## Hypothesis testing our correlation {.smaller}

How do we test whether the correlation we observed is significantly different from zero? Hypothesis test! 

. . . 

Step 1: Construct the null distribution, the sampling distribution of the null hypothesis

. . . 

```{r}
#| output-location: column

null_distribution_corr <- ratings %>% 
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "correlation") 

null_distribution_corr %>% head
```

. . . 

Step 2: How likley is our observed value under the null? Get a p-value.

. . . 

```{r}
#| output-location: column

p <- null_distribution_corr %>%
  get_p_value(
    obs_stat = obs_corr, 
    direction = "both")
p
```


## Hypothesis testing our correlation {.smaller}

How do we test whether the correlation we observed is significantly different from zero? Hypothesis test! 

. . . 

Step 3: Decide whether to reject the null! 

. . . 

```{r}
#| output-location: column

null_distribution_corr %>% 
  visualize() + 
  shade_p_value(
    obs_stat = obs_corr, 
    direction = "two-sided"
  ) 
```

. . . 

Interpret our p-value. Should we reject the null hypothesis?

# Model building 

Big picture overview of the model building process and the types of models we might encounter in our research. 

## Correlation is model building {.smaller}

Correlation is a simple case of model building, in which we use one value ($x$) to predict another ($y$). 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    annotate(x = 2.2, y = 6, geom = "text", label = "‚óè Data", size = 6) + 
    labs(x = "Frequency \n x", y = "y \n meanFamiliarity") +
    annotate(x = 7.5, y = 2.3, geom = "text", label = 'r == 0.482', size = 6, color = "red", parse = TRUE)
    

```


## Correlation is model building {.smaller}

Even more specifically ‚Äî formally, the **model specification** ‚Äî we are fitting the linear model $y = ax+b$, where $a$ and $b$ are free parameters. 

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "blue", method = "lm", se = FALSE) +
    annotate(x = 2.2, y = 6, geom = "text", label = "‚óè Data", size = 6) + 
    labs(x = "Frequency \n x", y = "y \n meanFamiliarity") + 
    annotate(x = 2.2, y = 5.5, geom = "text", label = "--- Model", size = 6, color = "blue") + 
    annotate(x = 7.5, y = 2.3, geom = "text", label = 'r == 0.482', size = 6, color = "red", parse = TRUE) 

```

::: 
::: {.column width=50%}

- Model specification: $y = ax + b$ 
- Estimate free parameters: $a$ and $b$
- Fitted model: $y = 0.39x + 2.02$

:::
::::

## How do we get $r = 0.48$ ? {.smaller}

The link between correlation and linear models is understood when we normalize our variables with a z-score. 

```{r}
#| echo: true
#| output-location: column

z_ratings <- ratings %>%
  select(Frequency, meanFamiliarity) %>%
  mutate(
    z_Freq = scale(Frequency), 
    z_meanFamil = scale(meanFamiliarity)
  )

z_ratings %>% head

```

- A z-score gets the number of standard deviations a data point is from the mean.

## Correlation is the slope of the model {.smaller}

Correlation is the slope of the line that best predicts $y$ from $x$ (after z-scoring)
```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 10

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "blue", method = "lm", se = FALSE) +
    annotate(x = 7.5, y = 2.3, geom = "text", label = 'r == 0.482', size = 10, color = "red", parse = TRUE) + 
    annotate(x = 7.0, y = 2.0, geom = "text", label = 'y == 0.39*x + 2.02', size = 10, color = "blue", parse = TRUE) +
    labs(title = "raw data") +
    theme_classic(base_size = 30)

ggplot(z_ratings, aes(x = z_Freq, y = z_meanFamil)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "red", method = "lm", se = FALSE) +
    annotate(x = 2, y = -1.5, geom = "text", label = 'r == 0.482', size = 10, color = "red", parse = TRUE) +
    annotate(x = 1.75, y = -1.8, geom = "text", label = 'y == 0.482*x + 0', size = 10, color = "blue", parse = TRUE) +
    labs(title = "z scored data") +
    theme_classic(base_size = 30)

```




## Model building overview 

- `Model specification` (this week): specify the functional form of the model.
- `Model fitting`: you have the form, how do you estimate the free parameters? 
- `Model accuracy`: you've estimated the parameters, how well does that model describe your data? 
- `Model reliability`: when you estimate the parameters, you want to quantify your uncertainty on your estimates 


## Types of models {.smaller}

. . . 


```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";

  "supervised learning" [style = filled]

}
``` 

## üí™ In-class Exercise {.smaller}

*Take a few minutes to try this yourself!*

Ask ChatGPT what type of model it is made with? 

. . . 

![](/assests/images/chatgpt-exercise.png)

## Supervised learning {.smaller} 

. . . 

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model [style = "invis"];
  x2 -> model [style = "invis"];
  x3 -> model [style = "invis"];
  x4 -> model [style = "invis"];
  x5 -> model [style = "invis"];

  model -> y [style = "invis"];  // Connect the model to y
}

```

## Supervised learning {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y [style = "invis"];  // Connect the model to y
}

```

## Supervised learning {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y;  // Connect the model to y
}

```

## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
 
  

  "regression" [style = filled]

}
```


## Regression v classification {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Column 4: regression v classify 
  r [label = "Regression (y is continuous) \n 1 3 4 2 3 4", shape = box];
  c [label = "Classification (y is discrete) \n yes/no, male/female/nonbinary", shape = box]; 

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y;  // Connect the model to y
  y -> r; 
  y -> c; 
}

```

## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
  "regression" -> "linear models";
  "regression" -> "nonlinear models";
  

  "linear models" [style = filled]

}
```

## Linear models 




:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false 
#| fig-height: 10

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    #labs(title = "Linear model") +
    theme_classic(base_size = 40) 

```

::: 
::: {.column width=50%}

- **Linear models** are models in which the output (y) is a weighted sum of the inputs 
- Easy to understand and fit
- $y=\sum_{i=1}^{n}w_ix_i$ 
- $y = ax + b$ is this!



:::
::::

## Linear model equation 

$y = ax + b$ *can be expressed* $y=\sum_{i=1}^{n}w_ix_i$ 

<!-- - implicit constant: $y=ax+b\mathbf{1}$  
- let $x_1=x$ and $x_2=\mathbf{1}$ 
- we have $y=ax_1 + bx_2$
- express $a$ and $b$ as weights: $a=w_1$ and $b=w_2$
- $y=w_1x_1 + w_2x_2$ where $w_1$ and $w_2$ are free parameters -->


## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
  "regression" -> "linear models";
  "regression" -> "nonlinear models";
  

  "nonlinear models" [style = filled]

}
```

## Nonlinear models {.smaller}

Output (y) cannot be expressed as a weighted sum of inputs($y=\sum_{i=1}^{n}w_ix_i$ ); pattern is better captured by more complex functions. (But often we can linearize them!)


```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 10

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    labs(title = "Linear model") +
    theme_classic(base_size = 36) 

# Generate x values (input variable) from 0 to 6 with 500 data points
x <- rnorm(500, mean = 0, sd = 1)

# Define the quadratic model y = ax^2 + bx + c with random noise
a <- 1.2   # Coefficient for x^2
b <- -0.5  # Coefficient for x
c <- 3.0   # Constant term
noise <- rnorm(length(x), mean = 0, sd = 1)  # Random noise

# Calculate y values (response variable)
y <- a * x^2 + b * x + c + noise

# Create a data frame with x, "Model", and y
data_nonlinear <- data.frame(x = x, Model = "Model", y = round(y, 2))

ggplot(data_nonlinear, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, formula = "y ~ poly(x, 2)", linewidth = 3) +
    labs(title = "Nonlinear model") +
    theme_classic(base_size = 36)

```

## üí™ In-class Exercise  {.smaller}

*Take a few minutes to try this yourself!*

Load the following data, which shows brain size and body weight for several different animals: 

::: {.nonincremental}
- [animal_brain_body_size.csv](/assests/csv/animal_brain_body_size.csv)
:::

Explore the data to specify the type of model we should use to predict brain size by body weight. 

::: {.nonincremental}
- Supervised or unsupervised? 
- Regression or classification?
- Linear or nonlinear? 
:::


## Model specification {.smaller}

Recall that `model specification` is one aspect of the model building process in which we select the form of the model (the type of model)

1. **Response variable ($y$):** Specify the variable you want to predict/explain (output). 
2. **Explanatory variables($x_i$):** Specify the variables that may explain the variation in the response (inputs). 
3. **Functional form:** Specify the relationship between the response and explanatory variables. *For linear models, we use the linear model equation!*
4. **Model terms:** Specify *how* to include your explanatory variables in the model (since they can be included in more than one way).

## Model specification {.smaller}

The following issues can also be considered part of the model specification process.

- **Model assumptions:** Check any assumptions underlying the model you selected (e.g. does the model assume the relationship is linear?).
- **Model complexity:** Simple models are easier to interpret but may not capture all complexities in the data. Complex models may suffer from overfitting the data or being difficult to interpret.

. . . 

*A well-specified model should be based on a clear understanding of the data, the underlying relationships, and the research question.*

## Specifying the functional form 

- Literally specifying the mathematical formula we're going to use to represent the relationship between our response and explanatory variables.
- We already know it: **linear models** are models in whch the response variable ($y$) is a weighted sum of the explanatory variables ($x_i$)
- $y=\sum_{i=1}^{n}w_ix_i$ 

## ü•∏ Aliases of $y=\sum_{i=1}^{n}w_ix_i$ 

The **linear model equation** can be expressed in many ways, but *they are all this same thing*

1. in **high school algebra**: $y=ax+b$. 
2. in **machine learning**: $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ 
3. in **statistics**: $y = Œ≤_0 + Œ≤_1x_1 + Œ≤_2x_2 + ... + Œ≤_nx_n + Œµ$
4. in **matrix** notation: $y = XŒ≤ + Œµ$

## Specify our first model 

To illustrate how this simple equation scales up to complex models, let's start with a simple case ("toy", "tractable"). 

```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 10

toy <- tibble(
  x = c(1, 2), 
  y = c(3, 5))
toy

toy %>%
ggplot(aes(x = x, y = y)) +
  geom_point(size=6) +
  theme_bw(base_size=45)+
  ylim(c(0,6)) +
  xlim(c(0,3))
```

## üí™ In class exercise 

Specify our model! 




# Swim Records 

Applied to a more complex problem 

## Specify a model for `SwimRecords`

How have world swim records in the 100m changed over time?

```{r}
library(mosaic)
glimpse(SwimRecords)
```

## üí™ In class exercise 

Plot the swim records data, then use your model specification worksheet to specify the model. 

## Response variable $y$ {.smaller}

What is the thing you are trying to understand?

```{r}
#| echo: false
theme_set(theme_bw(base_size = 26))
SwimRecords %>%
    ggplot(aes(y = time, x = year)) +
    geom_point() 

```

## Explanatory variable(s) $x_i$ {.smaller} 

What could **explain** the variation in your response variable? 

::: {.panel-tabset}

## year 

```{r}
#| echo: false

SwimRecords %>%
    ggplot(aes(y = time, x = year)) +
    geom_point() 

```

## year + sex

```{r}
#| echo: false

SwimRecords %>%
    ggplot(aes(y = time, x = year, shape= sex)) +
    geom_point() 

```

:::

## Functional form

- Linear model
- $y=\sum_{i=1}^{n}w_ix_i$ 


## Model terms {.smaller}

Model terms describe *how* to include our explanatory variables in our model formula ‚Äî there is more than one way!

1. Intercept
2. Main 
3. Interaction 
4. Transformation

## Intercept  {.smaller}

  - in R: `y ~ 1`, in eq: $y=w_1x_1$

```{r}
#| echo: false

model <- lm(time ~ 1, data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year)) +
    geom_point(aes(shape = sex), size = 2) +
    geom_point(
        size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```

## Main {.smaller}

::: {.panel-tabset}

## year

- in R: `y ~ 1 + year`, in eq: $y = w_1x_1 + w_2x_2$

```{r}
#| echo: false

model <- lm(time ~ 1 + year, data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year)) +
    geom_point(aes(shape = sex), size = 2) +
    geom_point(
        size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```

## sex 

- in R: `y ~ 1 + sex`, in eq: $y = w_1x_1 + w_2x_2$


```{r}
#| echo: false

model <- lm(time ~ sex, data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year, shape = sex)) +
    geom_point(size = 2) +
    geom_point(
        size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```

## year + sex

- in R: `y ~ 1 + year`, in eq: $y = w_1x_1 + w_2x_2 + w_3x_3$


```{r}
#| echo: false

model <- lm(time ~ year + sex, data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year, shape = sex)) +
    geom_point(size = 2) +
    geom_point(
       size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```

::: 

## Interaction {.smaller}

- in R: `y ~ 1 + year + gender + year:gender` 
    - or the short way: `y ~ 1 + year * gender`
- in eq: $y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4$ where $x_4$ is $x_2x_3$


```{r}
#| echo: false

model <- lm(time ~ year*sex, data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year, shape = sex)) +
    geom_point(size = 2) +
    geom_point(
       size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```

## Transformation {.smaller}

- in R: `y ~ 1 + year * sex + I(year^2)` 
- in eq: $y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4$ + $w_5x_5$ 
  - where $x_4$ is $x_2x_3$ and $x_5$ is $x_2^2$

```{r}
#| echo: false

model <- lm(time ~ year * sex + I(year^2), data = SwimRecords)
model_data <- SwimRecords %>%
    mutate(model_value = predict(model, SwimRecords))

SwimRecords %>%
    ggplot(aes(y = time, x = year, shape = sex)) +
    geom_point(size = 2) +
    geom_point(
       size = 3,
        color = "blue",
        data = model_data, 
        aes(y = model_value, x = year)) 

```


# THursday 

## Model building overview 

- `Model specification` (this week): specify the functional form of the model.
- `Model fitting`: you have the form, how do you estimate the free parameters? 
- `Model accuracy`: you've estimated the parameters, how well does that model describe your data? 
- `Model reliability`: when you estimate the parameters, you want to quantify your uncertainty on your estimates 

## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
  "regression" -> "linear models";
  "regression" -> "nonlinear models";
  "nonlinear models" -> "linearizable"; 
  "nonlinear models" -> "non-linearizable"
  
  "supervised learning" [style = filled]
  "regression" [style = filled]
  "linear models" [style = filled]
  "linearizable" [style = filled]

}
```

## Regression v classification {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Column 4: regression v classify 
  r [label = "Regression (y is continuous) \n 1 3 4 2 3 4", shape = box style = filled];
  c [label = "Classification (y is discrete) \n yes/no, male/female/nonbinary", shape = box]; 

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y;  // Connect the model to y
  y -> r; 
  y -> c; 

}

```

## Linear v Nonlinear models {.smaller}

- **Linear models** output ($y$) is a weighted sum of inputs ($x_i$): $y=\sum_{i=1}^{n}w_ix_i$ 
- **Nonlinear models** cannot be expressed as a weighted sum of inputs 
  - but usually we can linearize them!

```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 8

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    labs(title = "Linear model") +
    theme_classic(base_size = 36) 

# Generate x values (input variable) from 0 to 6 with 500 data points
x <- rnorm(500, mean = 0, sd = 1)

# Define the quadratic model y = ax^2 + bx + c with random noise
a <- 1.2   # Coefficient for x^2
b <- -0.5  # Coefficient for x
c <- 3.0   # Constant term
noise <- rnorm(length(x), mean = 0, sd = 1)  # Random noise

# Calculate y values (response variable)
y <- a * x^2 + b * x + c + noise

# Create a data frame with x, "Model", and y
data_nonlinear <- data.frame(x = x, Model = "Model", y = round(y, 2))

ggplot(data_nonlinear, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, formula = "y ~ poly(x, 2)", linewidth = 3) +
    labs(title = "Nonlinear model") +
    theme_classic(base_size = 36)

```



## Model specification {.smaller}

First aspect of the model building process in which we select the form of the model (the type of model)

1. **Response variable ($y$):** Specify the variable you want to predict/explain (output). 
2. **Explanatory variables($x_i$):** Specify the variables that may explain the variation in the response (inputs). 
3. **Functional form:** Specify the relationship between the response and explanatory variables: $y=\sum_{i=1}^{n}w_ix_i$ 
4. **Model terms:** Specify *how* to include your explanatory variables in the model (since they can be included in more than one way).

# "Toy" data
```r
toy_data <- tibble(
  x = c(1, 2, 3, 4, 5), 
  y = c(2, 6, 7, 12, 13)
)
```

## Explore the data {.smaller}

::: {.panel-tabset}

### Plot



```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

toy_data <- tibble(
  x = c(1, 2, 3, 4, 5), 
  y = c(2, 6, 7, 12, 13)
)

toy_data %>%
  ggplot(aes(x = x, y = y
  )) +
  geom_point() +
  theme_bw(base_size = 14) 

```


### Data

```{r}
#| echo: false
#| layout-ncol: 2

library(knitr)
kable(toy_data)
```


### Code

```r
toy_data <- tibble(
  x = c(1, 2, 3, 4, 5), 
  y = c(2, 6, 7, 12, 13)
)

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  theme_bw(base_size = 14) 

```
:::

## ¬†üí™In-class exercise 

Specify a model for the toy data. Then use `lm()` to fit the model. 

## Specify, fit, plot model {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(y ~ 1 + x, data = toy_data)

toy_data <- toy_data %>%
  mutate(with_formula = -0.4*1 + 2.8*x) %>%
  mutate(with_predict= predict(model, toy_data))

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{x}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = -0.4\cdot1 + 2.8\cdot x$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(toy_data)
```


## Code

```r
model <- lm(y ~ 1 + x, data = toy_data)

toy_data <- toy_data %>%
  mutate(with_formula = -0.4*1 + 2.8*x) %>%
  mutate(with_predict= predict(model, toy_data))

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

# Swim records 

```{r}
library(mosaic)
```

## One input {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 59.92*1) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 59.92 \cdot 1$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 59.92*1) %>% 
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## ¬†üí™In-class exercise 

Specify a model for the SwimRecords data that includes the intercept and the year. Use `lm()` to fit the model. Then use `predict()` to get the models prediction. 

If you have time, plot the data and model predictions with `ggplot()`.

## Two inputs {.smaller} 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 567.2420*1 + -0.2599*year) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 567.2420  \cdot \mathbf{1} + -0.2599 \cdot \mathbf{year}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 567.2420*1 + -0.2599*year) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


## Three inputs {.smaller}


::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year + sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(sex_numeric = case_when(
    sex == 'M' ~ 1,
    sex == 'F' ~ 0
  )) %>%
  mutate(with_formula = 555.7168*1 + -0.2515*year + -9.7980 *sex_numeric) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 555.7168 \cdot \mathbf{1} + -0.2515 \cdot \mathbf{year} + -9.7980 \cdot \mathbf{sex}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year + sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(sex_numeric = case_when(
    sex == 'M' ~ 1,
    sex == 'F' ~ 0
  )) %>%
  mutate(with_formula = 555.7168*1 + -0.2515*year + -9.7980 *sex_numeric) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## ¬†üí™In-class exercise 

We specified the model including intercept, year, and sex as follows: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex}$

Fit the model with `infer` this time. What are the following weights: 

- $w_1 = $
- $w_2 = $
- $w_3 = $




## Interaction {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year * sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex} + w_4\cdot\mathbf{year\times{sex}}$

```{r}
#| echo: false

model
```

Fitted model: <br>
\begin{align}
y = &697.3012 \cdot \mathbf{1} + -0.3240 \cdot \mathbf{year} + -302.4638 \cdot \mathbf{sex}\\
&+ 0.1499 \cdot \mathbf{year\times{sex}}
\end{align}

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year * sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


## Transformation {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year * sex + I(year^2), data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
\begin{align}
y = &w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex} \\
&+ w_4\cdot\mathbf{year\times{sex}} +w_5\cdot\mathbf{year}^2
\end{align}

```{r}
#| echo: false

model
```

Fitted model: <br>
\begin{align}
y = &11100 \cdot \mathbf{1} + -10.98 \cdot \mathbf{year} + -317.1 \cdot \mathbf{sex}\\
&+ 0.1575 \cdot \mathbf{year\times{sex}} + 0.002729 \cdot \mathbf{year}^2
\end{align}

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year * sex + I(year^2), data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## Linearizing nonlinear models {.smaller}

Two common ways:

1. **Expanding the input space** with polynomials. Polynomials can capture "bumps" or curves in the data. In this approach, we **add terms** to the model, like squares or cubes of the original variable. 
    - $y = w_1 + w_2x + w_3x^2$

2. **Transforming the data** involves applying mathematical functions to existing inputs to alter their scale or distributions. Taking the logarithm of a variable compresses its range and reduces skewness in the data (as in the brain size and body weight data). 
    - both output and input: $log(y) = w_1 + w_2 log(x)$ 
    - just input: $y = w_1 + w_2 log(x)$

# Plant heights (polynomials)

```r
poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')
```

## Polynomials {.smaller}

Polynomials capture "bumps" or curves in the data, and the number of these bumps depends on the **degree** of the polynomial. The higher the degree, the more complex the shape the polynomial can represent. 

![](/assests/images/polynomials.png)


## Degree 1 (Linear) {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure, data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_formula = 31.346*1 + 3.619*light_exposure) %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 31.346 \cdot 1 + 3.619 \cdot x$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r
poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure, data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_formula = 31.346*1 + 3.619*light_exposure) %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## ¬†üí™In-class exercise 

What happens if you specify the model without the intercept term in R? `y ~ x` instead of `y ~ 1 + x`? 

Try it with the `poly_plants` data and use 'lm()' or 'infer' to fit the model you specified. What do you noticed about the weights? 



## Degree 2 (Quadratic) {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x} + w_3 \cdot \mathbf{x}^2$


```{r}
#| echo: false

model
```

Fitted model: <br>
$y = -9.245 \cdot\mathbf{1} + 27.973 \cdot \mathbf{x} + -2.214  \cdot \mathbf{x}^2$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## Degree 3 (Cubic) {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2) + I(light_exposure^3), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x} + w_3 \cdot \mathbf{x}^2 + w_4 \cdot \mathbf{x}^3$


```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 8.7363 \cdot \mathbf{1} + 2.7276  \cdot \mathbf{x} + 3.7796 \cdot \mathbf{x}^2 + -0.3632  \cdot \mathbf{x}^3$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r
model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2) + I(light_exposure^3), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


# Brain size (log)

```r
brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv')
```


## Untransformed {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(brain_size_cc ~ 1 + body_size_kg, data = brain_data)

brain_data <- brain_data %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = body_size_kg, y = brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{body\_size\_kg}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 816.59014 \cdot\mathbf{1} + 0.05021 \cdot\mathbf{body\_size\_kg}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(brain_data)
```


## Code

```r
brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(brain_size_cc ~ 1 + body_size_kg, data = brain_data)

brain_data <- brain_data %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = body_size_kg, y = brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## Log transformed {.smaller}

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(log(brain_size_cc) ~ 1 + log(body_size_kg), data = brain_data)

brain_data <- brain_data %>%
  mutate(
    log_brain_size_cc = log(brain_size_cc), 
    log_body_size_kg = log(body_size_kg)
  ) %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = log_body_size_kg, y = log_brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$log(y) = w_1\cdot\mathbf{1} + w_2 \cdot log(\mathbf{body\_size\_kg})$

```{r}
#| echo: false

model
```

Fitted model: <br>
$log(y) = 2.2042\cdot\mathbf{1} + 0.6687 \cdot log(\mathbf{body\_size\_kg})$


:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(brain_data)
```


## Code

```r
model <- lm(log(brain_size_cc) ~ 1 + log(body_size_kg), data = brain_data)

brain_data <- brain_data %>%
  mutate(
    log_brain_size_cc = log(brain_size_cc), 
    log_body_size_kg = log(body_size_kg)
  ) %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = log_body_size_kg, y = log_brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


