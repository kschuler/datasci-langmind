---
title: "Sampling Distribution"
subtitle: "Data Science for Studying Language and the Mind"
author: Katie Schuler
date: 2025-09-16
echo: true
format: 
    revealjs:
        theme: dark
        incremental: true
        footer: "[https://kschuler.github.io/datasci-langmind/](/index.html)"
---

```{r}
#| echo: false
library(tidyverse)
library(infer)
theme_set(theme_classic(base_size = 20))
set.seed(123)

```


## Announcements



## You are `here` {.smaller} 

:::: {.columns}

::: {.column width="33%"}

##### Data science with R 
::: {.nonincremental}
- R basics
- Data visualization
- Data wrangling
:::
:::

::: {.column width="33%"}

##### Stats & Model building
::: {.nonincremental}
- `Sampling distribution`
- Hypothesis testing
- Model specification
- Model fitting 
- Model evaluation
:::
:::

::: {.column width="33%"}

##### More advanced 
::: {.nonincremental}

- Feature engineering 
- Classification
- Mixed-effect models
::: 
:::

::::

## Data science workflow 

![Data Science Workflow by R4DS](/assests/images/data-science-workflow.png)

# Attribution

Inspired by a MATLAB course Katie took by Kendrick Kay 

# Data 

Simulated from Ritchie et al 2018: 

. . . 

> Sex Differences in the Adult Human Brain: Evidence from 5216 UK Biobank Participants


# Descriptive statistics

## Dataset 

Suppose we measure a single quantity: `brain volume of human adults` (in cubic centemeters)

```{r}
#| echo: false 

data <- read_csv("http://kschuler.github.io/datasets/brain_volume.csv") %>% select(volume) 

head(data, 10) 
glimpse(data)

```

## Exploring a simple dataset {.smaller}

Each tick mark is one data point: one participant's brain volume

```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
    labs(x = "brain volume") 

```



## Visualize the distribution {.smaller}

Visualize the distribution of the data with a `histogram`

```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    labs(x = "brain volume")

```




## Measure of central tendency {.smaller}

Summarize the data with a single value: `mean`, a measure of where a central or typical value might fall

. . . 

```{r}
#| output-location: column
#| echo: false

sum_stats <- data %>% summarise(
    n = n(), 
    mean = mean(volume),
    sd = sd(volume))

  
```


```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
  geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +
  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```

## Measure of central tendency {.smaller}

Summarize the data with a single value: `mean`, a measure of where a central or typical value might fall


```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
  geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +
  annotate("text", x = 1250, y = 1200, label =round(sum_stats$mean, 2), size = 8) +
  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```




## Measure of variability {.smaller}

Summarize the spread of the data with `standard deviation`


```{r}
#| echo: false 

lower <- round(sum_stats$mean - sum_stats$sd, 2)
upper <- round(sum_stats$mean + sum_stats$sd, 2)


ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats$mean-sum_stats$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats$mean+sum_stats$sd, linetype = "dashed", size = 2) +

  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```

## Measure of variability {.smaller}

Summarize the spread of the data with `standard deviation`

. . . 

```{r}
#| echo: false 

lower <- round(sum_stats$mean - sum_stats$sd, 2)
upper <- round(sum_stats$mean + sum_stats$sd, 2)


ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats$mean-sum_stats$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats$mean+sum_stats$sd, linetype = "dashed", size = 2) +
    annotate("text", x = 980, y = 1200, label=lower, size = 8) +
    annotate("text", x = 1360, y = 1200, label=upper, size = 8) +

  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```

## Parametric statistics

Mean and sd are `parametric` summary statistics. They are given by the following equations:

:::: {.columns}
::: {.column width=50%}
$mean(x) = \bar{x} = \frac{\sum_{i=i}^{n} x_{i}}{n}$
:::
::: {.column width=50%}
sd($x$) = $\sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}$
:::
::::

## $mean(x) = \bar{x} = \frac{\sum_{i=i}^{n} x_{i}}{n}$

```{r}
#| echo: false 

lower <- round(sum_stats$mean - sum_stats$sd, 2)
upper <- round(sum_stats$mean + sum_stats$sd, 2)


ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```

## sd($x$) = $\sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}$


```{r}
#| echo: false 

lower <- round(sum_stats$mean - sum_stats$sd, 2)
upper <- round(sum_stats$mean + sum_stats$sd, 2)


ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats$mean-sum_stats$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats$mean+sum_stats$sd, linetype = "dashed", size = 2) +

  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```



## Nonparametric statistics

- Mean and sd are a good summary of the data when the distribution is `normal` (**gaussian**)
- But suppose our distribution is not normal.

## Visualize the distribution {.smaller}

Suppose we have a non-normal distribution 

. . . 

```{r}
#| echo: false
not_normal <- tibble(
    y = rexp(1000, rate = 0.5)
)

sum_stats_not <- not_normal %>%
    summarise(n = n(), mean = mean(y), sd = sd(y))

ggplot(not_normal, aes(x = y)) +
    geom_rug(alpha = 0.1) +
  #geom_histogram(color = "black", fill = "gray", bins = 9, alpha = 0.5) +
  #coord_cartesian(ylim = c(0, 60)) +
    labs(x = "y")

```


## Nonparametric statistics {.smaller}

`mean()` and `sd()` are not a good summary of central tendency and variability anymore.

```{r}
#| echo: false

ggplot(not_normal, aes(x = y)) +
    geom_rug(alpha = 0.1) +

  geom_histogram(color = "black", fill = "gray", bins = 15, alpha = 0.5) +
    geom_vline(xintercept = sum_stats_not$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats_not$mean-sum_stats_not$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats_not$mean+sum_stats_not$sd, linetype = "dashed", size = 2) +
    labs(x = "y")

```

## Median {.smaller}

Instead we can use the `median` as our measure of central tendency: the value below which 50% of the data points fall. 

```{r}
#| echo: false

np_sum_stats <- not_normal %>% summarise(
    n = n(), 
    median = median(y))
```



```{r}
#| echo: false 

ggplot(not_normal, aes(x = y)) +
    geom_rug(alpha = 0.1) +

  geom_histogram(color = "black", fill = "gray", bins = 15, alpha = 0.5) +
    geom_vline(xintercept = np_sum_stats$median, y = 1100, size = 2, color = "blue") +

#   geom_vline(xintercept = np_sum_stats$lower, linetype = "dashed", size = 2) +
#   geom_vline(xintercept = np_sum_stats$upper, linetype = "dashed", size = 2) +
 # coord_cartesian(ylim = c(0, 60)) +
    labs(x = "brain volume")

```
 
 

## IQR {.smaller}

And the interquartile range (`IQR`) as a measure of the spread in our data: the difference between the 25th and 75th percentiles (50% of the data fall between these values)

```{r}
#| echo: false

np_sum_stats <- not_normal %>% summarise(
    n = n(), 
    median = median(y),
    lower = quantile(y, 0.25),
    upper = quantile(y, 0.75) )

```

. . . 

```{r}
#| echo: false 

ggplot(not_normal, aes(x = y)) +
    geom_rug(alpha = 0.1) +

  geom_histogram(color = "black", fill = "gray", bins = 15, alpha = 0.5) +
    geom_vline(xintercept = np_sum_stats$median, y = 1100, size = 2, color = "blue") +

  geom_vline(xintercept = np_sum_stats$lower, linetype = "dashed", size = 2, color = "blue") +
  geom_vline(xintercept = np_sum_stats$upper, linetype = "dashed", size = 2, color = "blue") +
    labs(x = "brain volume")

```

## Coverage interval  {.smaller}

We can calculate any arbitrary coverage interval. In the sciences we often use the `95% coverage interval` — the difference between the 2.5 percentile and the 97.5 percentile — including all but 5% of the data. 

```{r}
#| echo: false

np_sum_stats <- not_normal %>% summarise(
    n = n(), 
    median = median(y),
    lower = quantile(y, 0.025),
    upper = quantile(y, 0.975) )

```

. . . 

```{r}
#| echo: false 

ggplot(not_normal, aes(x = y)) +
    geom_rug(alpha = 0.1) +

  geom_histogram(color = "black", fill = "gray", bins = 15, alpha = 0.5) +
    geom_vline(xintercept = np_sum_stats$median, y = 1100, size = 2, color = "blue") +

  geom_vline(xintercept = np_sum_stats$lower, linetype = "dashed", size = 2, color = "orange") +
  geom_vline(xintercept = np_sum_stats$upper, linetype = "dashed", size = 2, color = "orange") +
    labs(x = "brain volume")

```

# Probability distributions  {.smaller}

A mathematical function that describes the probability of observing different possible values of a variable (also called `probability density function`)

## Uniform probability distribution  {.smaller}

All possible values are equally likely

```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +

  geom_histogram(color = "black", fill = "gray", bins = 10, alpha = 0.5)  +
    coord_cartesian(ylim = c(0, 1))

```

## $p(x) = \frac{1}{max-min}$  {.smaller}

The probability density function for the uniform distribution is given by this equation (with two parameters:  `min` and `max`). 

```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "gray", bins = 10, alpha = 0.5)  +
  geom_density(color = "red", size = 2) +
  coord_cartesian(ylim = c(0, 1))
```
<!-- 
:::: {.columns}
::: {.column width=50%}
```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 10, alpha = 0.5) 
```

:::

::: {.column width=50%}


```{r}
uniform_sample %>% summarise(
    min = min(y), 
    max = max(y), 
    prob = 1/(max - min))
```



height of prob density func
```{r}
dunif(4, min = 1, max = 10)
```


prob less than given value
```{r}
punif(4, min = 1, max = 10)
```

:::
:::: -->





## Gaussian (normal) probability distribution {.smaller}

One of the most useful probability distributions for our purposes is the `Gaussian (or Normal) distribution` 

```{r}
#| echo: false

normal_sample <- tibble(
  y = rnorm(1000, mean = 0, sd = 1)
)

# plot uniform with histogram 
ggplot(normal_sample, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 12, alpha = 0.5) 
```


## $p(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)$ {.smaller}

The probability density function for the Gaussian distribution is given by the following equation, with the parameters $\mu$ (mean) and $\sigma$ (standard deviation). 

```{r}
#| echo: false

# plot uniform with density
ggplot(normal_sample, aes(x = y)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "gray", bins = 12, alpha = 0.5)  +
  geom_density(color = "red", size = 2) + 
  coord_cartesian(ylim = c(0, 1))

```

## Gaussian (normal) probability distribution {.smaller}

![](/assests/images/gaussian-distribution-rule.jpg)

- When computing the mean and standard deviation of a set of data, we are implicitly fitting a Gaussian distribution to the data. 


# Sampling variability

## The population {.smaller}

When measuring some quantity, we are usually interested in knowning something about the `population`: the mean brain volume of Penn undergrads (the **parameter**)


```{r}
#| echo: false
set.seed(45)
population <- tibble(
  volume = rnorm(11250, mean = 1219, sd = 161),
  sex = NA)

  p_stats <- population %>% summarise(n = n(), mean = mean(volume), sd = sd(volume))


population %>%
  ggplot(aes(x = volume)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
        geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
      annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
      labs( y = "count", x = "brain volume", caption = "n = 11,250")


```


## The sample  {.smaller}

But we only have a small `sample` of the  population: maybe we can measure the brain volume of 100 students

```{r}
#| echo: false
sample <- population %>% 
  specify(response = volume) %>% 
  rep_slice_sample(n = 100, reps = 1000) 

s_stats <- sample %>%
  group_by(replicate) %>%
  summarise(mean = mean(volume))

use <- s_stats %>% summarise(
  min = min(mean), rep_min = which.min(mean), max = max(mean), rep_max = which.max(mean))

   population %>%
  ggplot(aes(x = volume)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_min), color = "navy") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +      annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample n = 100")

 

```

## Sampling variability {.smaller}

Any statistic we compute from a random sample we've collected (`parameter estimate`) will be subject to sampling variability and will differ from that statistics computed on the entire population (`parameter`)

```{r}
#| echo: false

 population %>%
  ggplot(aes(x = volume)) +
  #coord_cartesian(xlim = c(800, 1600)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_min), color = "navy") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
         annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
    annotate(geom = "text", x = use$min - 250, y = 1600, label = "parameter estimate →", color = "navy", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample n = 100")
 

```


## Sampling variability {.smaller}

If we took another sample of 100 students, our parameter estimate would be different. 

```{r}
#| echo: false

 population %>%
  ggplot(aes(x = volume)) +
  #coord_cartesian(xlim = c(800, 1600)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_max), color = "darkred") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
         annotate(geom = "text", x = p_stats$mean - 200, y = 1600, label = "parameter →", size = 6) +
    annotate(geom = "text", x = use$max + 250, y = 1600, label = "← parameter estimate 2", color = "darkred", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample #2 n = 100")

```


## Sampling distribution {.smaller}

The `sampling distribution` is the probability distribution of values our parameter estimate can take on. Constructed by taking a random sample, computing stat of interest, and repeating many times.

```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_histogram(bins = 18, color = "gray") +
    #  geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +
    #    geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

    labs(title = "Sampling distribution of mean brain volume", x = "parameter estimate (mean)") 

```

## Sampling distribution {.smaller}

Our first sample was on the low end of possible mean brain volume. 

```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_histogram(bins = 18, color = "gray") +
    #  geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +
       geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

    labs(title = "Sampling distribution of mean brain volume", x = "parameter estimate (mean)") 

```

## Sampling distribution {.smaller}

Our second sample was on the high end of possible mean brain volume. 

```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_histogram(bins = 18, color = "gray") +
     geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +
       geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

    labs(title = "Sampling distribution of mean brain volume", x = "parameter estimate (mean)") 

```



## Quantifying sampling variability {.smaller}

The `spread` of the sampling distribution indicates how the parameter estimate will vary from different random samples. 


```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_rug() +
    geom_histogram(bins = 18, color = "gray")  
```

## Quantifying sampling variability {.smaller}

We can quantify the spread (express our `uncertainty` on our parameter estimate) in two ways. 

- Parametrically, by compute the `standard error`
- Nonparametrically, by constructing a `confidence interval` 


## Quantifying sampling variability {.smaller}

One way is to compute the standard deviation of the sampling distribution, which has a special name: the `standard error`


- The standard error is given by the following equation, where $\sigma$ is the standard deviation of the population and $n$ is the sample size. 
- $\frac{\sigma}{n}$
- In practice, the standard deviation of the population is unknown, so we use the standard deviation of the *sample* as an estimate. 

## Standard error is parametric {.smaller}

- Standard error is a `parametric` statistic because we assume a gaussian probaiblity distribution and compute standard error based on what happens theoretically when we sample from that theoretical distribution. 
- $\frac{\sigma}{n}$

. . . 

```{r}
#| echo: false

se <- s_stats %>% summarise(samp_mean = mean(mean), samp_sd = sd(mean), se = samp_sd/sqrt(length(mean)))

theoretical_ci <- se %>% mutate(lower = samp_mean-samp_sd, upper = samp_mean + samp_sd) %>% select(lower, upper)

ggplot(s_stats, aes(x = mean)) +
    #geom_rug() +
    geom_histogram(bins = 18, color = "white", fill = "white")  +
    shade_ci(theoretical_ci) +
    labs(y = "count")

```

## Quantifying sampling variability {.smaller}

Another way is to construct a `confidence interval`

```{r}
#| echo: false

ci <- s_stats %>%
  summarise(lower = quantile(mean, 0.157), upper = quantile(mean, .843))


ggplot(s_stats, aes(x = mean)) +
    geom_rug() +
    geom_histogram(bins = 18, color = "gray")  +
    shade_ci(ci) +
    labs(y = "count")

```

## Practical considerations 

- We don't have access to the entire population
- We can (usually) only do our experiment once
- So, in practice we only have `one` sample 


# Bootstrapping

To construct the sampling distribution

## Bootstrapping 

Instead of assuming a parametric probability distributon, we use the data themselves to approximate the underlying distribution: we `sample our sample`!


##  Bootsrapping with `infer` 

> The objective of this package is to perform statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework

```r
install.packages("infer")`
```

## Let's create some data {.smaller}

Suppose we collect a sample of 100 subjects and find their mean brain volume is 1200 cubic cm and sd is 100: 

```{r}
#| echo: true
#| output-location: column
# get a sample to work with as our "data"
sample1 <- tibble(
  subject_id = 1:100,
  volume = rnorm(100, mean = 1200, sd = 100)
)

sample1 %>% head(10)
```

## Generate the sampling distribution {.smaller}

Generate the sampling distribution with `specify()`, `generate()`, and `calculate()`

```{r}
#| output-location: column

bootstrap_distribution <- sample1  %>% 
  specify(response = volume) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean")

bootstrap_distribution
```

## Visualize the bootstrap distribution {.smaller}

Visualize the bootstrap distribution you generated with `visualize()`

```{r}
#| output-location: column
bootstrap_distribution %>% 
  visualize()
```

- Visualize is a shortcut function to ggplot!

## Quantify the spread with `se` {.smaller}

Quantify the spread of the bootstrapped sampling distributon with `get_confidence_interval()`, and set the type to `se` for standard error. 

```{r}
#| output-location: column
se_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type = "se",
    point_estimate = mean(sample1$volume)
  )

se_bootstrap

```

. . . 


```{r}
#| output-location: column
bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = se_bootstrap
  )
```

## Quantify the spread with `ci` {.smaller}

Quantify the spread of the sampling distributon with `get_confidence_interval`, and set the type to `percentile` for **confidence interval**

```{r}
#| output-location: column
ci_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type  ="percentile", 
    level = 0.95
  )

ci_bootstrap
```

. . . 

```{r}
#| output-location: column

bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = ci_bootstrap
  )
```

## Questions? 

- Let's stop there, and work through some more demos in our next lecture!

# Thursday

# Plan for today 

Tuesday's lecture was conceptual. Today we will demo those concepts to try to understand them better. 

# Descriptive statistics

Let's first try to understand descriptive statistics a bit better by using a toy dataset. 

## Creating a 'toy' dataset {.smaller}

Suppose we create a tibble that measures a single quantity: how many minutes your instructor was late to class for 10 days. 

```{r}
#| echo: true
#| output-location: column

(data <- tibble(
  late_minutes = c(1, 2, 2, 3, 4, 2, 5, 3, 3)
))


```

. . . 

We can sort the values with `arrange` to get a quick sense of the data 
```{r}
#| echo: true
#| output-location: column

data %>% 
  arrange(late_minutes) 


```

## Summarize with descriptive statistics {.smaller}

Recall tha twe can summarize (or describe) a set of data with `descriptive statistics` (aka summary statistics). There are three we typically use: 

| Measure | Stats  | Describes | 
| --- | --- | --- |
| Central tendency | mean, median, mode | a central or typical value | 
| Variability | variance, standard deviation, range, IQR | dispersion or spread of values |
| Frequency  distribution | count | how frequently different values occur 

## Frequency distribution {.smaller}

We can create a visual summary of our dataset with a histogram, which plots the `frequency distribution` of the data. 

```{r}
#| echo: true
#| output-location: column

data %>%
  ggplot(aes(x = late_minutes)) + 
  geom_histogram() 

```

. . . 

We can also get a count with `group_by()` and `tally()`

```{r}
#| echo: true
#| output-location: column

data %>% 
  group_by(late_minutes) %>%
  tally() 

```

## Central tendency {.smaller}

Measure of central tendency describe where a central or typical value might fall 

```{r}
#| echo: true
#| output-location: column

data %>%
  ggplot(aes(x = late_minutes)) + 
  geom_histogram() 

```

. . . 

We can get these with `group_by()` and `summarise()`

```{r}
#| echo: true
#| output-location: column

data %>% 
  summarise(
    n = n(), 
    mean = mean(late_minutes), 
    median = median(late_minutes)
    )

```


## Variability {.smaller}

Measures of variability which describe the dispersion or spread of values

```{r}
#| echo: true
#| output-location: column

data %>%
  ggplot(aes(x = late_minutes)) + 
  geom_histogram() 

```

. . . 

We can also get these with `group_by()` and `summarise()`

```{r}
#| echo: true
#| output-location: column

data %>% 
  summarise(
    n = n(), 
    sd = sd(late_minutes), 
    min = min(late_minutes), 
    max = max(late_minutes), 
    lower = quantile(late_minutes, 0.25),
    upper = quantile(late_minutes, 0.75) 
    )

```

## Parametric descriptive statistics {.smaller}

Some statistics are considered `parametric` because they make assumptions about the distribution of the data (we can compute them theoretically from parameters)

## Mean {.smaller}

The mean is one example of a parametric descriptive statistic, where $x_{i}$ is the $i$-th data point and $n$ is the total number of data points

$mean(x) = \bar{x} = \frac{\sum_{i=i}^{n} x_{i}}{n}$

. . . 

```{r}
#| echo: true
#| output-location: column

mean(data$late_minutes)


```

. . . 

We can compute this equation by hand to see that the results are the same. 
```{r}
#| echo: true
#| output-location: column

sum(data$late_minutes)/length(data$late_minutes)


```

## Standard deviation {.smaller}

Standard deviation is another paramteric descriptive statistic where $x_{i}$ is the $i$-th data point, $n$ is the total number of data points, and $\bar{x}$ is the mean. 

. . . 

$sd(x) = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}$


```{r}
#| echo: true
#| output-location: column

sd(data$late_minutes)


```

. . . 

We can compute this by hand as well, to see how it happens under the hood of `sd()`

```{r}
#| echo: true
#| output-location: column

mean_late <- mean(data$late_minutes)
(sd_by_hand <- data %>% 
  mutate(dev = late_minutes - mean_late) %>%
  mutate(sq_dev = dev^2))

```

. . . 

```{r}
#| echo: true
#| output-location: column

sd_by_hand %>% 
  summarise(
    n = n(), 
    n_minus_1 = n-1, 
    sum_sq_dev = sum(sq_dev), 
    by_hand_sd = sqrt(sum_sq_dev/n_minus_1))

```

## Visualize the mean and sd {.smaller}

How do we visualize the mean and sd on our histogram? 

. . . 

First get the summary statistics with `summarise()` 

```{r}
#| echo: true
#| output-location: column

(sum_stats <- data %>%
  summarise(
    n = n(), 
    mean = mean(late_minutes), 
    sd = sd(late_minutes), 
    lower_sd = mean - sd, 
    upper_sd = mean + sd
  ))


```

. . . 

Then use those values to plot with `geom_vline()`. 

```{r}
#| echo: true
#| output-location: column

data %>%
  ggplot(aes(x = late_minutes)) + 
  geom_histogram() +
  geom_vline(
    xintercept = sum_stats$mean, 
    color = "blue"
    ) + 
  geom_vline(
    xintercept = sum_stats$lower_sd,
    color = "red"
    ) +
  geom_vline(
    xintercept = sum_stats$upper_sd, 
    color = "red"
    )

```



## Nonparametric descriptive statistics {.smaller}

Other statistics are considered `nonparametric`, because thy make minimal assumptions about the distribution of the data (we can compute them theoretically from parameters)

## Median {.smaller}

The mean is the value below which 50% of the data fall. 

. . . 


```{r}
#| echo: true
#| output-location: column

median(data$late_minutes) 


```

. . . 

We can check whether this is accurate by sorting our data 

```{r}
#| echo: true
#| output-location: column

data %>% 
  arrange(late_minutes) 


```



## Inter-quartile range (IQR) {.smaller}

The difference between the 25th and 75th percentiles. We can compute these values with the `quantile()` function. 


```{r}
#| echo: true
#| output-location: column

data %>%
  summarise(
    iqr_lower = quantile(late_minutes, 0.25), 
    iqr_upper = quantile(late_minutes, 0.75)
  )


```

. . . 


Again, we can check whether this is accurate by sorting our data 

```{r}
#| echo: true
#| output-location: column

data %>% 
  arrange(late_minutes) 


```


## Coverage intervals {.smaller}

The IQR is also called the 50% coverage interval (because 50% of the data fall in this range). We can calculate any artibrary coverage interval with `quantile()`

. . . 

```{r}
#| echo: true
#| output-location: column

data %>%
  summarise(
    iqr_lower = quantile(late_minutes, 0.025), 
    iqr_upper = quantile(late_minutes, 0.975)
  )


```

. . . 

Again, we can check whether this is accurate by sorting our data 

```{r}
#| echo: true
#| output-location: column

data %>% 
  arrange(late_minutes) 


```

## Visualize the median and coverage intervals {.smaller}

We can visualize these statistics on our histograms in the same way we did mean and sd:

. . . 

First get the summary statistics with `summarise()` 

```{r}
#| echo: true
#| output-location: column

(sum_stats <- data %>%
  summarise(
    n = n(), 
    median = median(late_minutes), 
      ci_lower = quantile(late_minutes, 0.025), 
    ci_upper = quantile(late_minutes, 0.975)
  ))


```

. . . 

Then use those values to plot with `geom_vline()`. 

```{r}
#| echo: true
#| output-location: column

data %>%
  ggplot(aes(x = late_minutes)) + 
  geom_histogram() +
  geom_vline(
    xintercept = sum_stats$mean, 
    color = "blue"
    ) + 
  geom_vline(
    xintercept = sum_stats$ci_lower,
    color = "red"
    ) +
  geom_vline(
    xintercept = sum_stats$ci_upper, 
    color = "red"
    )

```


# Probability distributions 

A probability distribution is a mathematical function of one (or more) variables that describes the likelihood of observing any specific set of values for the variables. 

## R's functions for parametric probability distributions {.smaller}

| function | params | returns | 
| - | -- | ----- | 
| `d*()` | depends on * | height of the probability density function at the given values | 
| `p*()` | depends on * | **cumulative density function** (probability that a random number from the distribution will be less than the given values) | 
| `q*()` | depends on * | value whose cumulative distribution matches the probaiblity (**inverse of p**) |
| `r*()` | depends on * | returns n **random numbers** generated from the distribution

## Uniform distribution {.smaller}

The uniform distribution is the simplest probability distribution, where all values are equally likely. The probability density function for the uniform distribution is given by this equation (with two parameters:  `min` and `max`). 

$p(x) = \frac{1}{max-min}$

```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "gray", bins = 10, alpha = 0.5)  +
  geom_density(color = "red", size = 2) +
  coord_cartesian(ylim = c(0, 1))
```


## R's functions for Gaussian distribution {.smaller}

We just use `norm` (normal) to stand in for the `*`

| function | params | returns | 
| - | -- | ----- | 
| `dnorm()` |x,  mean, sd | height of the probability density function at the given values | 
| `pnorm()` | q, mean, sd | **cumulative density function** (probability that a random number from the distribution will be less than the given values) | 
| `qnorm()` | p, mean, sd | value whose cumulative distribution matches the probaiblity (**inverse of p**) |
| `rnorm()` | n, mean, sd | returns n **random numbers** generated from the distribution


## `rnorm()` to sample from the distribution {.smaller}

 `rnorm(n, mean, sd)`: returns n **random numbers** generated from the distribution

```{r}
#| echo: true
#| output-location: column

(normy <- tibble(
  x = rnorm(1000, mean = 0, sd = 1)
))
```


## `dnorm(x, mean, sd)`  {.smaller}

Returns the height of the probability density function at the given values 

```{r}
#| echo: true
#| output-location: column

ggplot(normy, aes(x = x )) +
  geom_density()

```

. . . 

```{r}
#| echo: true
#| output-location: column

dnorm(2, mean = 0, sd = 1)

```


## `pnorm(q, mean, sd)`  {.smaller}

Returns the **cumulative density function** (probability that a random number from the distribution will be less than the given values) 

```{r}
#| echo: true
#| output-location: column

ggplot(normy, aes(x = x )) +
  geom_density()

```

. . . 

```{r}
#| echo: true
#| output-location: column

pnorm(3, mean = 0, sd = 1)
```


```{r}
#| echo: true
#| output-location: column

pnorm(-2, mean = 0, sd = 1)
```


## `qnorm(p, mean, sd)` {.smaller}

Returns the value whose cumulative distribution matches the probability 

```{r}
#| echo: true
#| output-location: column

ggplot(normy, aes(x = x )) +
  geom_density()

```

. . . 

```{r}
#| echo: true
#| output-location: column

qnorm(0.99, mean = 0, sd = 1)
```


```{r}
#| echo: true
#| output-location: column

qnorm(0.02, mean = 0, sd = 1)
```


## Using other distributions {.smaller}

Change the function's suffix (the * in `r*()`) to another distribution and pass the parameters that define that distribution. 

. . . 

 `runif(n, min, max)`: returns n **random numbers** generated from the distribution

```{r}
#| echo: true
#| output-location: column

(uni<- tibble(
  x = runif(1000, min= 0, max = 1)
))
```

. . . 

But remember, this only works for **paramteric** probability distributions (those defined by particular paramters)

# Sampling distribution and bootstrapping

Let's do a walk through from start to finish 

## The parameter {.smaller}

Generate data for the brain volume of the 28201 grad and undergrad students at UPenn and compute the parameter of interest (mean brain volume)

```{r}
#| echo: true
#| output-location: column

(population <- tibble(
  subject_id = 1:28201, 
  volume = rnorm(28201, mean = 1200, sd = 100)
))

```


## The parameter estimate {.smaller}

Now take a realistic sample of 100 students and compute the paramter estimate (mean brain volume on our sample)

```{r}
#| echo: true
#| output-location: column

(sample <- population %>%
  sample_n(100))

```


## But our parameter estimate is noisy {.smaller}

- When measuring a quantity, the measurement will be different each time. We attribute this variability to *noise*, any factor that contributes variability in measurement. 
- Any statistic (e.g. mean) that we compute on a random sample is subject to variability as well; we need to distrust (to some degree) this statistic. 
- To indicate our uncertainty on our parameter estimate, we can use 
  - standard error (the standard deviation of the sampling distribution; parametric)
  - confidence intervals (the nonparametric approach to quantify spread)


## Bootstrap the sampling distribution {.smaller}

Use `infer` to construct the probability distribution of the values our parameter estimate can take on (the sampling distribution). 

```{r}
#| echo: true
#| output-location: column

(bootstrap_distribution <- sample  %>% 
  specify(response = volume) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean"))

```



## Standard error {.smaller}

Recall that standard error is the standard deviation of the sampling distribution. It indicaes about how far away the true population might be. 

```{r}
#| echo: true
#| output-location: column

(se_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type = "se",
    point_estimate = mean(sample$volume)
  ))

```


```{r}
#| echo: true
#| output-location: column

bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = se_bootstrap
  )

```



## Confidence interval {.smaller}

Confidence intervals are the nonparameteric approach to the standard error: if the distribution is Gaussian, +/- 1 standard error gives the 68% confidence internval and +/- 2 gives the 95% confidence interval. 

```{r}
#| echo: true
#| output-location: column

(ci_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type = "percentile",
   level = 0.68
  ))

```


```{r}
#| echo: true
#| output-location: column

bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = ci_bootstrap
  )

```


## Interpreting confidence intervals {.smaller}

```{r}
#| echo: true
#| output-location: column

bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = ci_bootstrap
  )

```

- **technical interpretation**: if we repeated our experiment, we can expect the X% of the time, the true population parameter will be contained within the X% confidence interval. 
- **looser interpretation**: we can use the confidence interval as an indicator of our uncertainty in our parameter estimate. 

# Questions? 

