---
title: "Model specification"
author: 
    - Katie Schuler
---


## Correlation as model building 

Correlation is actually a simple case of model building in which we use one value ($x$) to predict another ($y$). Specifically, we are fitting the linear model $y = ax + b$, where $a$ and $b$ are free parameters.  Here, $y$ is known as the **response variable** (the value we are trying to predict) and $x$ is the **explanatory variable** (the one that we are attempting to *explain* the response variable with). 

- After z-scoring our variables, the correlation between $x$ and $y$ is equal to the slope of the line that best predicts $y$ from $x$. 

## Model building overview

Ideally we want to understand statistical modeling beyond the simple case of correlation. What if we have more than one explanatory variable? What if the relationship between variables is not linear? To address model building more broadly, it is helpful to think of building any model as a four-step process. We'll treat each of these separately over the coming weeks. The goal for today is to get a big picture overview of the model building process and the types of models we might encounter in our research. 

- **Model specification** (this week): what is the form?
- **Model fitting** (this week); you have the form, how do you guess the free parameters? 
- **Model accuracy** (after break): you've estimated the parameters, how well does that model describe your data? 
- **Model reliability** (after break): when you estimate the parameters, there is some uncertainty on them

## Types of models

Model specification involves deciding which type of model we'd like to apply. We will mostly apply linear models in this class, but it's useful to first have a conceptual overview of the types of models we *could* apply. 

![Types of models](/assests/images/typseofmodels-1.png)



### Supervised vs unsupervised

As a starting point, we can divide statistical models into two types of learning (so called because we are trying to "learn" something about the data): 

- In **supervised learning**, we want to predict an output (or response) variable based on one or more input (or explanatory) variables. We call this supervised learning because both the input and output variables are known (sometimes this is called "labeled" data), and we are trying to learn the relationship between them. Linear regression is an example of a supervised learning model.
- In **unsupervised learning**, there is no specific output variable that we are trying to predict. Instead, the model's objective is to discover the underlying structure or patterns in the data. We call this unsupervised learning because only the input data is available (sometimes this is called "unlabeled" data); the model is trying to identify relationships in the data without being "supervised" by an outcome variable. PCA and cluster analysis are examples of unsupervised learning. 
- There are other machine learning approaches beyond these, like semi-supervised learning (combining both labeled and unlabeled data) and reinforcement learning (learning through trial and error based on rewards or penalties). But in this course we will focus on supervised learning models. 

### Regression v classification
 
Regression and classification are both types of supervised learning models — using one or more input variables to predict an output variable. The only difference between them is in type of output variable: 

- **Regression** is used when we want to predict a continuous output, meaning it is a number that can take on any value within a range (e.g. height, weight, response time)
- **Classification** is used when we want to predict a categorical output, meaning it falls into specific classes or categories (e.g. true/false, yes/no, male/female/nonbinary). We cover this during advanced model building.

![Types of models](/assests/images/typesofmodels.png)



### Linear v nonlinear regression

There are many types of regression models, but we can simplify by dividing them into two main types of models: 

- In **linear regression**, the relationship between the explanatory variable(s) and response variable is represented by a linear equation (a straight line graphed on a two-dimensional plane).
- **Nonlinear regression** is useful when the data does not follow a linear pattern, and the relationship between the variables is better captured by more complex functions (e.g. a curve or any other nonlinear shape). Nonlinear regression models can be further divided into two types: 
  - We can **linearize** a nonlinear model by applying a mathematical transformation to make it look like a linear equation (e.g. log, square root, etc). We can fit a linearized model just like a linear model, but the prediction of the model is not linear with respect to x. 
  - Sometimes, no matter what transformation you apply, you cannot acheive a linear form. These types of models are referred to as **nonlinearizable** nonlinear models and are beyond the scope of this class!


## Model specification
Recall from last week that **model specification** is one aspect of the model building process. It involves selecting the functional form of the model (the type of model) and choosing which variables to include. When specifying a model, you'll need to make the following decisions: 

1. **Response variable ($y$):** Choose the variable you want to predict or explain (output). 
2. **Explanatory variables($x_n$):** Choose the variables that may explain the variation in the response variable (inputs). 
3. **Functional form:** Specify the functional relationship between the response and explanatory variables. For linear models, the relationship is linear, and we use the linear model equation as our functional form!
4. **Model terms:** Choose which model terms to include, which is another way of saying that you need to decide *how* to include your explanatory variables in the model (since they can be included in more than one way).

In addition to the decisions above, the following issues can also be considered part of the model specification process. But we will consider these in future weeks.

- **Model assumptions:** Check any assumptions underlying the model you selected (e.g. does the model assume the relationship is linear?).
- **Model complexity:** Simple models are easier to interpret but may not capture all complexities in the data. Complex models may suffer from overfitting the data or being difficult to interpret.

A well-specified model should be based on a clear understanding of the data, the underlying relationships, and the research question. 

### Response variable, $y$

Choosing the response variable is usually straightforward once you've clearly defined your research question: what is the thing you are trying to understand? You also need to make sure whatever you've selected is something you can measure (or has already been measured!)

- In the swim records example, we are trying to explain variation in record times, so we choose record time as our $y$. In the brain size example we are trying to explain variation in brain sizes, so we choose brain size as our $y$.
- Remember from last time that we can use regression for continuous response variables (numbers) but we need to use classification if the response variable is categorical (categories or levels). 

### Explanatory variables, $x_n$

Choosing which explanatory variables to include requires a bit more careful consideration. It's one part using your knowledge about the domain you are studying and one part exploratory data analysis!

- One extreme would be to include just one explanatory variable: the obvious one based on your research question. In the swim records example, we want to understand how swim records change over time, so we should definitely include `time` as an explanatory variable.  But this model is **underspecified**.  We need to consider other variables that have the *potential* to explain variation in our response variable, even if they are not of direct interest. For example, some variation in swim record times can likely be explained by the swimmer's gender, so we should include `gender` as an explanatory variable in our model.
- Importantly, we do not include every explanatory variable we can think of! We want to explain the variation in our response variable without building too complex a model or overfitting the data (**overspecifying**). We'll go into more detail about this in future lectures. For now, just remember you're Goldilocks: you want the explanatory variables in the model to explain just the right amount of variation.

### Functional form

When specifying the functional form of a model, we're literally specifying the mathematical formula we're going to use to represent the relationship between our response and explanatory variables. **Linear models** -- our focus in this class -- are models in which the response variable (output) is a weighted sum of the explanatory variables (inputs). In other words, there is a linear relationship between the response variable and the explanatory variables. The **linear model equation** can be expressed in many ways (which can be confusing!). Here are four different ways of representing the formula for the linear model (the functional form), to emphasize that *they are all the same thing*.

1. In **high school algebra**, the linear model equation is represented as the equation of a straight line. $y$ is the response variable, $x$ is the explanatory variable, $a$ is the slope of the line (the relationship between $x$ and $y$) and $b$ is the y-intercept (the value of $y$ when $x$ is zero). You have (hopefully!) already encountered this equation.
  - $y=ax+b$.  
2. In **machine learning**, the linear model equation is usually represented as a weighted sum of input variables. Note that the only changes are that we refer to the free parameters as weights ($w_n$) instead of $a$ and $b$ (to emphasize these are the weights the model learns) and the ability to add more than one input ($x_1, x_2, ...x_n$ instead of just $x$):
  - $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ 
3. In **statistics**, the linear model equation is also represented as a weighted sum of input variables, except we call the weights "regression coefficients" ($\beta_n$) and we add an error term to account for unexplained variability ($\varepsilon$):
  - $y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n + ε$
4. In **matrix** notation, the linear model equation is represented as a dot product of vectors. This is just a more compact representation of the statistics (or machine learning) way, often used in linear algebra and statistics. $X$ is the matrix containing the values of the explanatory variables, $β$ is the vector of regression coefficients, and $\varepsilon$ is the vector of error terms.
  - $y = Xβ + ε$
 
### Model terms

We've specified our response and explanatory variables and the functional form of our model. Now we need to specify the model terms. Model terms describe *how* to include the explanatory variables in our model (they can be included in more than one way!) There are four kinds of terms: (1) **intercept**, (2) **main**, (3) **interaction**, and (4) **transformation**.

1. The **intercept** term, $\beta_0$, is a constant (not variable) capturing the typical value of the response variable when all explanatory variables are zero. It allows the model to have an offset from the origin, so it is also called the "offset" parameter in some fields. Unless it makes sense for our response variable to be zero when all other variables are zero (it rarely does!) we should include the intercept term.
  - in R: `y ~ 1`
  - in eq: $y=\beta_0 + \varepsilon$

2. **Main** terms (AKA **main effects**) represent the effect of each explanatory variable on the response variable directly. In other words, how does the response variable change as a result of changes in a given explanatory variable, when all other explanatory variables are zero? Each main term corresponds to one explanatory variable and is included in the model as a single term ($\beta_nx_n$). We can add as many explanatory variables as we like to the model:
  - in R: `y ~ 1 + year + gender` 
  - in eq: $y = \beta_0 + \beta_1x_1 + \beta_2x_2$
  - Note that can include **categorical explanatory variables** like gender, we just need to find a way to represent the same information numerically, since linear models require numerical inputs.




3. **Interaction** terms allow us to express that the effect of one explanatory variable on the response variable is different at different values of another explanatory variable. For example, in the swim records data, the effect of gender on record times changes over year (or said another way, the effect of year on record times is different for men and women). We are still describing how variation in the response variable is explained by one or more explanatory variables, we're just describing how two (or more) variables *combine* to influence the response. In the linear model equation, we add a term to the model in which we multiply the values of the interacting variables.
  - in R: `y ~ 1 + year + gender + year:gender` 
      - or the short way: `y ~ 1 + year * gender`
  - in eq: $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2$



4. **Transformation** terms allow us to modify the explanatory variables to accommodate nonlinear relationships with the response variable. Some of the most common transformations are $x^2$, $\sqrt{x}$, and $log(x)$. Note that $x$ must be a quantitative variable: we can't transform categorical variables. In the swim records example, squaring the year term ($x_1^2$) allowed our model to have a curve shape. But notice that this makes it seem like record times are slowing down after 1990. This is obviously not the case — records inherently only get faster! — but models lack common sense, and there is no easy math way to tell our model to "be curvy, but also never slope upward". 
  - in R: `y ~ sq(year) * gender`
  - eq: $y = \beta_0 + \beta_1x_1^2 + \beta_2x_2 + \beta_3x_1^2x_2$



## Examples!

Now that we've covered the terminology and concepts, let's apply model specification to some real models. 

```{r}
#| message: false

library(tidyverse)
library(mosaic)
```


### "Toy" data 


Let's start with the simplest possible example, a dataset with two data points. Suppose you record how many days you study over the next 5 days. On day 1, you study for 2 hours. On day 2, you study for 6 hours and so on. Your dataset might look something like this. 

::: {.panel-tabset}

### Plot



```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

toy_data <- tibble(
  x = c(1, 2, 3, 4, 5), 
  y = c(2, 6, 7, 12, 13)
)

toy_data %>%
  ggplot(aes(x = x, y = y
  )) +
  geom_point() +
  theme_bw(base_size = 14) 

```


### Data

```{r}
#| echo: false
#| layout-ncol: 2

library(knitr)
kable(toy_data)
```


### Code

```r
toy_data <- tibble(
  x = c(1, 2, 3, 4, 5), 
  y = c(2, 6, 7, 12, 13)
)

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  theme_bw(base_size = 14) 

```
:::

1. **Specify our response variable**, $y$: the response variable (🥸 data, output, prediction) is the variable you are trying to predict or explain with your model.
    - `y`

2. **Specify explantory variables**, $x_i$: the explanatory variables (🥸 regressors, inputs, predictors) are the predictors in your data that could help explain the response variable. Our data has only one possible: 
    - `x` 

3. **Specify the functional form**: the functional form describes the relationship between the response and explanatory variables with a mathematical expresson. In a linear model, we express this relationship as a weighted sum of inputs: 
    - $y=\sum_{i=1}^{n}w_ix_i$

4. **Specify model terms**: here we need to specify exactly *how* to express our explanatory variables in our functional form. The actual variables and constants that will be included in the model. There are four kinds of terms: (1) intercept, (2) main, (3) interaction, and (4) transformation. Here we have the simplest case of an intercept and one main term (no interactions or transformations necessary)
    - $y = w_1\mathbf{1} + w_2x_2$
    - in R: `y ~ 1 + x`

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(y ~ 1 + x, data = toy_data)

toy_data <- toy_data %>%
  mutate(with_formula = -0.4*1 + 2.8*x) %>%
  mutate(with_predict= predict(model, toy_data))

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{x}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = -0.4\cdot1 + 2.8\cdot x$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(toy_data)
```


## Code

```r
model <- lm(y ~ 1 + x, data = toy_data)

toy_data <- toy_data %>%
  mutate(with_formula = -0.4*1 + 2.8*x) %>%
  mutate(with_predict= predict(model, toy_data))

toy_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

### Swim records

#### One input

If our model has a single input, it is likely the intercept term, a constant (not variable) capturing the typical value of the response variable when all explanatory variables are zero. 


::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 59.92*1) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 59.92 \cdot 1$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 59.92*1) %>% 
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


#### Two inputs

We can add another term to our model represnting the effect of year on record time. This is a **main term** or **main effect**, which represents the effect of each explanatory variable on the response variable directly. In other words, how does record time change as a result of changes in year, when all other explanatory variables are zero? 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 567.2420*1 + -0.2599*year) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 567.2420  \cdot \mathbf{1} + -0.2599 \cdot \mathbf{year}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_formula = 567.2420*1 + -0.2599*year) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


#### Three inputs

We can see that the previous model allowed us to capture the effect of year on record time, but we still have some unexplained variation. We can include sex in the model to capture the difference in record times by sex. 


::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year + sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(sex_numeric = case_when(
    sex == 'M' ~ 1,
    sex == 'F' ~ 0
  )) %>%
  mutate(with_formula = 555.7168*1 + -0.2515*year + -9.7980 *sex_numeric) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 555.7168 \cdot \mathbf{1} + -0.2515 \cdot \mathbf{year} + -9.7980 \cdot \mathbf{sex}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year + sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(sex_numeric = case_when(
    sex == 'M' ~ 1,
    sex == 'F' ~ 0
  )) %>%
  mutate(with_formula = 555.7168*1 + -0.2515*year + -9.7980 *sex_numeric) %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


#### Interaction 

Notice that the previous model now gets us record times getting faster by year, and different predictions for men and women (women have slower times). But this is missing another relationship we can see in our data: that women are getting faster, faster. To express that the effect of one explanatory variable on the response variable is different at different values of another explanatory variable (e.g. the effect of year on record times is different for men and women), we add a term to the model in which we multiply the values of the interacting variables.

*We could say that we "expand the input space" of the model, since we add terms to capture the interaction*

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year * sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex} + w_4\cdot\mathbf{year\times{sex}}$

```{r}
#| echo: false

model
```

Fitted model: <br>
\begin{align}
y = &697.3012 \cdot \mathbf{1} + -0.3240 \cdot \mathbf{year} + -302.4638 \cdot \mathbf{sex}\\
&+ 0.1499 \cdot \mathbf{year\times{sex}}
\end{align}

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year * sex, data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


#### Transformation 

Now our model is doing a great job at predicting our data, but there may be more we want to do. For example, we can see that the model is not predicting women very well around the year 2000 (it is predicting they will be faster than they are). If we want to allow the model to have a curve shape, capturing that women gained on men for a while, but are no slowing down, we can add a term to the model in which we square the year. This allows us to capture this nonlinear curve or bend in the data (more on this for polynomials in the next section). 

*but notice that the model is fitting the data well, but still behaving a bit non-sensical toward the 2000s, predicint that record times are getter slower! Impossible!)*

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="40%"}


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3

model <- lm(time ~ 1 + year * sex + I(year^2), data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="60%"}

Model specification: <br>
\begin{align}
y = &w_1\cdot\mathbf{1} + w_2\cdot\mathbf{year} + w_3\cdot\mathbf{sex} \\
&+ w_4\cdot\mathbf{year\times{sex}} +w_5\cdot\mathbf{year}^2
\end{align}

```{r}
#| echo: false

model
```

Fitted model: <br>
\begin{align}
y = &11100 \cdot \mathbf{1} + -10.98 \cdot \mathbf{year} + -317.1 \cdot \mathbf{sex}\\
&+ 0.1575 \cdot \mathbf{year\times{sex}} + 0.002729 \cdot \mathbf{year}^2
\end{align}

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


library(knitr)
kable(SwimRecords_predict)
```


## Code

```r
model <- lm(time ~ 1 + year * sex + I(year^2), data = SwimRecords)

SwimRecords_predict <- SwimRecords %>%
  mutate(with_predict= predict(model, SwimRecords))

SwimRecords_predict %>%
  ggplot(aes(x = year, y = time, shape = sex)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

## Linearizing nonlinear models 

When you want to linearlize a nonlinear model, you're trying to fit a linear model to data that doesn't naturally follow a straight line. There are two common ways to approach this: 

1. **Expanding the input space** with polynomials. Polynomials can capture "bumps" or curves in the data. In this approach, we **add terms** to the model, like squares or cubes of the original variable. 
    - $y = w_1 + w_2x + w_3x^2$

2. **Transforming the data** involves applying mathematical functions to existing inputs to alter their scale or distributions. Common transformations include taking the logarithm or square root. Taking the logarithm of a variable compresses its range and reduces skewness in the data (as in the brain size and body weight data). 
    - both output and input: $log(y) = w_1 + w_2 log(x)$ 
    - just input: $y = w_1 + w_2 log(x)$

### Plant heights (polynomials)

Polynomials capture "bumps" or curves in the data, and the number of these bumps depends on the **degree** of the polynomial. The higher the degree, the more complex the shape the polynomial can represent. 

![](/assests/images/polynomials.png)


- **Degree 1 (Linear)**: A straight line. There are no bumps or curves. The relationship between the predictor and the response is either increasing or decreasing at a constant rate.
- **Degree 2 (Quadratic)**: A single bump or curve. The graph is either a U-shape (bowl) or an upside-down U-shape (hill), meaning it can capture one turning point.
- **Degree 3 (Cubic)**: Can capture two bumps (or one “S” shaped curve). The graph can have two turning points, meaning it can start by increasing, then decrease, and increase again (or the opposite).
- **Degree 4 (Quartic)**: Can capture three bumps or changes in direction. The graph can have up to three turning points, allowing for more complex shapes and curves in the data.

#### Degree 1 (Linear)

Remember, a **Degree 1 (Linear)** is a straight line. There are no bumps or curves. The relationship between the predictor and the response is either increasing or decreasing at a constant rate. This doesn't seem to capture the relationship between light_exposure and plant_height in our data. 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure, data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_formula = 31.346*1 + 3.619*light_exposure) %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 31.346 \cdot 1 + 3.619 \cdot x$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r
poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure, data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_formula = 31.346*1 + 3.619*light_exposure) %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

#### Degree 2 (Quadratic)

In a **Degree 2 (Quadratic)** polynomial, we can express a single bump or curve. The graph is either a U-shape (bowl) or an upside-down U-shape (hill), meaning it can capture one turning point. This provides a better fit for our data, allow us to express the light exposure goes up and then back down again. But it looks like there is another "bump" in the data, going back upward around light exposure of 1 or 2.

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x} + w_3 \cdot \mathbf{x}^2$


```{r}
#| echo: false

model
```

Fitted model: <br>
$y = -9.245 \cdot\mathbf{1} + 27.973 \cdot \mathbf{x} + -2.214  \cdot \mathbf{x}^2$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

#### Degree 3 (Cubic)

In a **Degree 3 (Cubic)** polynomial, we can capture two bumps (or one “S” shaped curve). The graph can have two turning points, meaning it can start by increasing, then decrease, and increase again (or the opposite). This captures the data quite nicely. 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

poly_plants <- read_csv('https://kschuler.github.io/datasci/assests/csv/polynomial_plants.csv')

model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2) + I(light_exposure^3), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2 \cdot \mathbf{x} + w_3 \cdot \mathbf{x}^2 + w_4 \cdot \mathbf{x}^3$


```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 8.7363 \cdot \mathbf{1} + 2.7276  \cdot \mathbf{x} + 3.7796 \cdot \mathbf{x}^2 + -0.3632  \cdot \mathbf{x}^3$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(poly_plants)
```


## Code

```r
model <- lm(plant_height ~ 1 + light_exposure + I(light_exposure^2) + I(light_exposure^3), data = poly_plants)

poly_plants <- poly_plants %>%
  mutate(with_predict= predict(model, poly_plants))

poly_plants %>%
  ggplot(aes(x = light_exposure, y = plant_height)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::


### Brain size (log)


#### Untransformed

When we have a nonlinear relationship, as here, we could just try to fit a linear model to the untransformed data. It *techincally* works — there is no math reason that prevents us from fitting this model — but we can see that it is a very bad description of the data. 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(brain_size_cc ~ 1 + body_size_kg, data = brain_data)

brain_data <- brain_data %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = body_size_kg, y = brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$y = w_1\cdot\mathbf{1} + w_2\cdot\mathbf{body\_size\_kg}$

```{r}
#| echo: false

model
```

Fitted model: <br>
$y = 816.59014 \cdot\mathbf{1} + 0.05021 \cdot\mathbf{body\_size\_kg}$

:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(brain_data)
```


## Code

```r
brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(brain_size_cc ~ 1 + body_size_kg, data = brain_data)

brain_data <- brain_data %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = body_size_kg, y = brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::

#### Log transformed

We can apply a log transform directly in the model specification provided to R. This works great, but if we try to plot the fitted model on untransformed data (e.g. if we use brain_size_cc and body_size_kg as our y and x aesthetics) something doesn't seem quite right. Instead, we should plot the data transformed to log as well, so the model predictions match the data. 

::: {.panel-tabset}

## Plot

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false
#| message: false
#| fig-height: 3
#| fig-width: 3

brain_data <- read_csv('https://kschuler.github.io/datasci/assests/csv/animal_brain_body_size.csv') %>%
  rename(brain_size_cc = `Brain Size (cc)`, body_size_kg = `Body Size (kg)`)

model <- lm(log(brain_size_cc) ~ 1 + log(body_size_kg), data = brain_data)

brain_data <- brain_data %>%
  mutate(
    log_brain_size_cc = log(brain_size_cc), 
    log_body_size_kg = log(body_size_kg)
  ) %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = log_body_size_kg, y = log_brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```

:::
::: {.column width="50%"}

Model specification: <br>
$log(y) = w_1\cdot\mathbf{1} + w_2 \cdot log(\mathbf{body\_size\_kg})$

```{r}
#| echo: false

model
```

Fitted model: <br>
$log(y) = 2.2042\cdot\mathbf{1} + 0.6687 \cdot log(\mathbf{body\_size\_kg})$


:::
::::


## Data

```{r}
#| echo: false
#| layout-ncol: 2


kable(brain_data)
```


## Code

```r
model <- lm(log(brain_size_cc) ~ 1 + log(body_size_kg), data = brain_data)

brain_data <- brain_data %>%
  mutate(
    log_brain_size_cc = log(brain_size_cc), 
    log_body_size_kg = log(body_size_kg)
  ) %>%
  mutate(with_predict= predict(model, brain_data))

brain_data %>%
  ggplot(aes(x = log_body_size_kg, y = log_brain_size_cc)) +
  geom_point() +
  geom_line(aes(y = with_predict), color = "blue") +
  theme_bw(base_size = 14) 

```
:::




## Further reading

- [Ch 6: Language of models](https://dtkaplan.github.io/SM2-bookdown/language-of-models.html) in Statistical Modeling

